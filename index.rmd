

---
title: "Algorithmic Fairness "
subtitle: "PHI420/PHI591 - Fall 21"
author: "Marcello Di Bello  - mdibello@asu.edu"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 2
    toc: TRUE
---    





<!-- 
OTHER INITIAL SETTING 

---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 2
    toc: TRUE
---    
    
--->


<!-- 
OTHER INITIAL SETTING PRETTY HTML
---
pagetitle: "Algorithmic Fairness"
title: "Algorithmic Fairness"
subtitle: ""
author: "Marcello Di Bello"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    navbar: yes
    toc: yes
    toc_depth: 3
    toc_float: no
---
-->



<!---
---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  html_document:
    toc: true
    navbar: yes
    toc_depth: 3
    toc_float: yes
    theme: cosmo
  pdf_document:
    toc: false
  highlight: zenburn    
---
--->



<script>
   $(document).ready(function() {
     $head = $('#header');
     $head.prepend('<img src=\"algo-fair-logo.png\" style=\"float: right;width: 300px;height: 220px;\"/>')
   });
</script>


<!---
<img src="data-ethics-logo.jpg" style="position:absolute;top:0px;right:0px;" />
--->

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	             cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
      font-family:'Avenir Next';
  }

</style>



<!--- 
ADDED STYLES/ FONT SIZSES
<style type="text/css">
body{ /* Normal  */
      font-size: 18px;
  }
  
 
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>
-->


Th	3-5:50 PM in Tempe, [COOR](https://www.asu.edu/map/interactive/?psCode=COOR) 3301  

Office hours: TBA

# Topic

Public and private sector entities rely on algorithms to streamline decisions about 
healthcare, loans, social services, sentencing and more. 
This course examines the ongoing debate among computer scientists, 
legal scholars and philosophers about the fairness of algorithmic decision-making.  What does it mean for an algorithm to be fair? How does algorithmic fairness relate to other ideas such as anti-discrimination and fair equality of opportunity? Can fairness be measured quantitatively? What other values and goals should inform the design of ethical algorithms, such as minimizing the harm toward disadvantaged minorities or respecting each person's individuality?


# Objectives

This course is meant for advanced undergraduates and graduates students. 
You will become familiar with different conceptions of algorithmic fairness,
such as predictive parity, classification parity and counterfactual fairness, as well as 
explore connections between algorithmic
fairness, anti-discrimination law and philosophical concepts such as fair equality of opportunity. 
You will sharpen your critical thinking skills, in reading  and  writing, 
for the analysis of new technologies and the ethical issues they pose. You will 
read academic papers in different disciplines---computer science, economics, law, sociology 
and philosophy---and develop an appreciation for differences in scholarship 
across disciplines.

# Course Materials

Readings and other course materials are available 
at no cost on this website. For readings covered 
by copyright, please check the Canvas page for this course.  
Course materials are divided into "essential" and "additional". 
You are only expected to study the essential ones, but I 
reccomend that you have a look at the 
additional materials for at least one 
or two weeks. 




# Requirements


## Participation

Please attend class regularly and participate 
(**10% of our grade**). This is a "seminar style" course. You are 
expected to take an active role in the discussions. Please study the assigned 
materials **before class** and be ready to discuss them. 


## Assignments

In addition, 
you should write ten 
Pass/Fail précis as well as three graded essays or 
a research paper (**90% of your grade**).

### Pass/fail 

Every week please write a **one-page précis** of one of the papers assigned for that week. 
The précis should describe: (a) topic of the paper; 
(b) main thesis (or main theses, if there are more than one); 
(c) supporting arguments; (d) objections to these arguments, complications or 
difficulties that the author considers (if any). Submit 
your précis each week through Canvas **before the beginning of class**. 
You should receive a PASS in **at least ten précis**, or else a full letter grade 
will be subtracted from your final grade (i.e. A would become B; B would become C; etc.)


### Essays 

There will be three main graded assignments 
for this course, **5 pages each**. 

**1**. After doing independent research, collect 
and summarize **two stories**. One should describe how an algorithm has positively 
impacted people's lives and another how an algorithm has negatively impacted people's lives. The two stories need not be---and often will not be---about the same algorithm, but the same algorithm could have both positive and negative effects. Please convey to the reader why you found the two stories meaningful. 

**2**. Write **an exploratory essay** that describes very carefully (a) ProPublica's accusation that COMPAS is racially biased and Northpointe's response; (b) assess what further technical, practical and theoretical questions about algorithmic fairness must be addressed in order to settle the debate. Please be precise.

**3**. Write a **philosophical argumentative essay** that attempts to answer the question "What is Algorithmic Fainess?" In answering this question, I reccomend that you compare and contrast the formal notions of algorithmic fairness (such as predictive parity, classification parity, causal fairness, etc.) with either (i) the economic literature on discrimination, (ii) equal protection jurprisprudence, (iii) the philosophical literature on fairness and equality. Please always defend the claims you make with careful and reasoned arguments. Check out these [guidelines](http://www.jimpryor.net/teaching/guidelines/writing.html) on how to write 
a philosophical argumentative essay.

### Research paper 

If you are a grad student or advanced undergrad with research experience, 
you may combine the three 5 page essays into one 
**15-20 page** research paper. The research paper should engage closely 
with a subset of the course materials including the additional ones. 
It is neither necessary nor reccomended that you use materials outside those already 
included in the course materials. 

*Please come talk to me before you start working on the research paper.* 

# Schedule 


## Wk 1 - 8/19

**Introduction**

The question of algorithmic fairness is technical (how do algorithms work?), practical (how do algorithms impact people's lives?) and theoretical (what does "fairness" mean?). On the technical side, we will survey key concepts, such as algorithm, machine learning, supervised v. unsupervised learning, regression, classification v. clustering. This survey will be informal. Those who wish to go deeper may consult the additional materials. 
On the practical side, we will read stories about people whose lives were affected, often adversely, by algorithms. On the theoretical side, we will brainstorm ideas about what we mean by "fairness" and how an algorithm can be unfair. We will explore several definitions of fairness more deeply later in the course. 

*Key readings and materials*

- O'Neil, [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/) - Introduction

- Kearns and Roth, [The Ethical Algorithm](https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205) - Chp. 1 

- [A Gentle Introduction to Machine Learning](https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)

- Eubanks, [Automating Inequality](https://www.amazon.com/Automating-Inequality-High-Tech-Profile-Police/dp/1250215781/) - Chp. 4


*Additional readings and materials*

- Rogers and Girolami, [A First Course in Machine Learning](https://www.amazon.com/Course-Machine-Learning-Pattern-Recognition-ebook/dp/B01N7ZEBK8), Chp. 1

- Nielsen, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - [Chp. 1](http://neuralnetworksanddeeplearning.com/chap1.html)

- [What Is Backpropagation Really Doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U)


## Wk 2 - 08/26

**ProPublica v. Northpointe**

To examine more closely the interplay between technical, theoretical and practical 
questions, we will look at a case study, the algorithm
[COMPAS](https://en.wikipedia.org/wiki/COMPAS_(software)) 
used in criminal justice to help judges make decisions about bail, preventative detention and sentencing. The website of investigative journalism [ProPublica](https://www.propublica.org/) argued that COMPAS is biased against black people. Northpointe (now [Equivant](https://www.equivant.com/)), the company that designed COMPAS, 
rejected the accusation claiming that COMPAS is racially fair. They both provided numbers to back up their claims.  So is COMPAS racially biased or not? The crux of the matter here seems to be a disagreement 
about the meaning of "fairness".

To better understand this debate, you should be familiar with a bif of probability theory and machine learning, in particular, notions such as false positive, false negative, sensitivity, specificity, Area Under the Curve (AUC) and confusion matrix.  We will review these notions as needed.

*Key readings and materials*

- Machine Learning Fundamentals: 
    - [Sensitivity and Specificity](https://www.youtube.com/watch?v=vP06aMoz4v8)
    - [ROC and AUC](https://www.youtube.com/watch?v=4jRBRDbJemM)
    - [Confusion Matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o)

- ProPublica, [Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

-  Dieterich,  Mendoza and  Brennan, [COMPAS risk scales: Demonstrating Accuracy Equity and Predictive Parity Performance of the COMPAS Risk Scales in Broward County](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) 

*Additional readings and materials*

- ProPublica, [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

- Flores, Bechtel and  Lowenkamp. [False Positives, False Negatives, and False Analyses: A rejoinder to "Machine Bias"](https://www.uscourts.gov/federal-probation-journal/2016/09/false-positives-false-negatives-and-false-analyses-rejoinder)

- [Parole Denied: One Man's Fight Against a COMPAS Risk Assessment](https://www.youtube.com/watch?v=UySPgihj70E)


## Wk 3  - 9/2

**(Mis)guided by Data**

Let's take a step back and examine the basis of algorithmic decision-making: data.
Though incomplete and never truly conclusive, data 
can guide our decisions, often benefitting people. For example, data have been playing a key role in the response to the COVID-19 Pandemic (see,among others, Abedin et al, [Using Data to Inform the
COVID-19 Policy Response](https://www.theigc.org/wp-content/uploads/2020/11/Abedin-et-al-2020-Policy-paper.pdf))

<!---

Max Roser, Hannah Ritchie and Esteban Ortiz-Ospina, [Coronavirus Disease (COVID-19) - Statistics and Research](https://ourworldindata.org/coronavirus)). 

-->

Another example, in the context of criminal justice, is the data-driven Public Safety Assessment ([PSA](https://advancingpretrial.org/psa/about/))
<!--
(https://www.psapretrial.org/)) 
--->
algorithm, now widely used in several jurisdictions in the United States to help judges make pre-trial decisions. According to a [report](https://njcourts.gov/courts/criminal/reform.html) from New Jersey, pre-trial jail population descrease significantly after PSA was adopted. The ACLU of New Jersey [endorsed](https://www.aclu-nj.org/theissues/criminaljustice/pretrial-justice-reform) 
the use of the PSA algorithm because it has the potential to end the pre-trial system of money and bail that disproportionately harms the poor. 

Although data can be helpful to make decisions, they can also be biased, partial or incomplete. Inadequate data can be harmful especially for already disadvantaged minorities. An important problem is the use of "proxies" in the historical data. That is, the algorithm is not trained on the actual outcomes (which often cannot be known), but on proxies of the actual outcome (say 'arrest' is used as proxy for 'crime'). We will see a few examples of biased, partial and incomplete data and their harmful effects on people. 

*Key readings and materials*

- Kearns and Roth, [The Ethical Algorithm](https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205) - Chp. 3 (only pp. 57-64)

- Hardt, [How Big Data Is Unfair](https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de)

- [Technical Flaws of Pretrial Risk Assessments Raise Grave Concerns](https://cyber.harvard.edu/story/2019-07/technical-flaws-pretrial-risk-assessments-raise-grave-concerns)

- Perez, [Invisible Women: Data Bias in a World Designed for Men](https://www.amazon.com/Invisible-Women-Data-World-Designed/dp/1419729071) - Chp. 8


*Additional readings and materials*

- Caliskan, Bryson, and Narayanan, [Semantics Derived Automatically from Language Corpora Contain Human-Like Biases](https://arxiv.org/abs/1608.07187)

- Lazer, Kennedy, King, and Vespignani, [The Parable of Google flu: Traps in Big Data Analysis](https://gking.harvard.edu/files/gking/files/0314policyforumff.pdf)

-  Buolamwini, [Algorithms Aren’t Racist. Your Skin Is just too Dark](https://hackernoon.com/algorithms-arent-racist-your-skin-is-just-too-dark-4ed31a7304b8)


## Wk 4 - 9/9

**Feedback Loops**

Another problem with data is that decisions based on inadequate data magnify errors on a large scale especially when they are automated. Even unbiased data can produce pernicious feedback loops. We will exmaine examples of feedback loops in algorithms used in education and policing. 


*Key readings and materials*

- O'Neil, [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/) - Chps. 3, 4 and 5 

-  Lum and Isaac, [To Predict and Serve?](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x)


*Additional readings and materials*


-  Ensign, Friedler, Neville, Scheidegger and Venkata, [Runaway Feedback Loops in Predictive policing](https://arxiv.org/abs/1706.09847) - [video](https://www.youtube.com/watch?v=9C6epG-Wyuw)

- Harcourt, [Against Prediction](https://www.amazon.com/Against-Prediction-Profiling-Punishing-Actuarial/dp/0226316149) - [Chp. 5](https://www.tau.ac.il/law/events/15-12-06/C-Chapter5.pdf)


**First essay due**

## Wk 5 - 9/16

**Meanings of Fairness** 

We have not yet asked the fundamental questions. 
What does it mean for algorithmic decisions to be fair? How is fairness different 
from accuracy? Computer scientists have formulated several 
different metrics of algorithmic fairness. 
By one count, there are as many as 21 definitions. 
Three of the most important definitions out there 
are called: *demographic parity*,  *classification parity* and *predictive parity*. 
We will examine these definitions and their relationship to accuracy. 


*Key readings and materials*

- Kearns and Roth, *The Ethical Algorithm*, Chp. 3 

-  Corbett-Davies and  Goel. [The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning](https://arxiv.org/abs/1808.00023) - [video](https://www.youtube.com/watch?v=Qc6fYWPkDtU)

*Additional readings and materials*

-  Narayanan, [21 Fairness Definitions and Their Politics](https://www.youtube.com/watch?v=jIXIuYdnyyk)

-  Barocas, Hardt and Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) - [Chp. 2](https://fairmlbook.org/classification.html) 

<<<<<<< HEAD
=======
- Kusner, Loftus, Russell and Silva, [Counterfactual Fairness](https://arxiv.org/abs/1703.06856)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd

## Wk 6 - 09/23

**Impossibility Theorems** 


It turns out  it is mathematically impossible, under realistic conditions,  for an algorithm
to satisfy different conceptions of fairness at the same time. These impossibility theorems 
led scholars to consider trade-offs between different conceptions of fairness or 
be skeptical toward existing definitions.

*Key readings and materials*

- Chouldechova, [Fair Prediction with Disparate Impact](https://arxiv.org/pdf/1703.00056.pdf)

- Hellman, [Measuring Algorithmic Fairness](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3418528)


*Additional readings and materials*

- Grant, [Is It Impossible to Be Fair?](https://jainfamilyinstitute.github.io/algorithmic-fairness/)
<<<<<<< HEAD

- Hedden, [On Statistical Criteria of Algorithmic Fairness](https://philpapers.org/rec/HEDOSC) - [video](https://www.youtube.com/watch?v=QEFK0l4wgaw)

-  Berk,  Heidari,  Jabbari,  Kearns and  Roth, [Fairness in Criminal Justice Risk Assessments: The State of the Art](https://doi.org/10.1177/0049124118782533) 

-  Kleinberg,  Mullainathan and Raghavan, [Inherent Trade-offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807) - [video](https://www.youtube.com/watch?v=K7i_tnflZ64) 
=======

- Hedden, [On Statistical Criteria of Algorithmic Fairness](https://philpapers.org/rec/HEDOSC) - [video](https://www.youtube.com/watch?v=QEFK0l4wgaw)

-  Berk,  Heidari,  Jabbari,  Kearns and  Roth, [Fairness in Criminal Justice Risk Assessments: The State of the Art](https://doi.org/10.1177/0049124118782533) 

-  Kleinberg,  Mullainathan and Raghavan, [Inherent Trade-offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807) - [video](https://www.youtube.com/watch?v=K7i_tnflZ64) 

- Ben Green, [The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness](https://www.benzevgreen.com/wp-content/uploads/2020/01/20-fat-risk.pdf)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd


## Wk 7 - 09/30 

**Fair Risk Assessment** 

The debate about algorithmic fairness is nothing new. Since the development of statistical methods in insurance and banking, claims of racial and gender discrimination have been advanced against risk assessment tools used to price loans and insurance policies. We will review this history to understand analogies and differences between  fair risk assessment in the 20th century and algorithmic fairness in the 21st century. 

*Key readings and materials*

- Rodrigo Ochigame, [The Long History of Algorithmic Fairness](https://phenomenalworld.org/analysis/long-history-algorithmic-fairness)  



**Second essay due**


## Wk 8 - 10/7

**Structural Unfairness** 

The definitions  of algorithmic fairness we have seen so far describe the extent to which *actual* rates of error are unevenly (and thus unfairly) allocated across different socially relevant groups. Scholars in computer science have also used more sophisticated definitions to formulate a *counterfactual* or *causal* definition of algorithmic fairness. These definitions attempt to capture the structural dimension of racial discrimination (as in the expression "structural racism"). Other scholars in law, sociology and philosophy have argued that these attempt are unsatisfactory. 

*Key readings and materials*

- Lily Hu, Disparate Causes, [Part I](https://phenomenalworld.org/analysis/disparate-causes-i) and [Part II](https://phenomenalworld.org/analysis/disparate-causes-pt-ii)

-  Kohler-Hausmann, [Eddie Murphy and the Dangers of Counterfactual Causal Thinking About Detecting Racial Discrimination](https://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1374&context=nulr)


*Additional readings and materials*

-  Barocas, Hardt and Narayanan, [Fairness and Machine Learning](https://fairmlbook.org/) - [Chp 5: Causality](https://fairmlbook.org/causal.html)

<<<<<<< HEAD
-  Green, [The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness](https://www.benzevgreen.com/wp-content/uploads/2020/01/20-fat-risk.pdf)

- Kusner, Loftus, Russell and Silva, [Counterfactual Fairness](https://arxiv.org/abs/1703.06856)

-  Chiappa and Gillam,  [Path-specific Counterfactual Fairness](https://arxiv.org/pdf/1802.08139.pdf)

=======
-  Chiappa and Gillam,  [Path-specific Counterfactual Fairness](https://arxiv.org/pdf/1802.08139.pdf)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd


## Wk 9 - 10/14

**Algorithmic Equal Protection**

Algorithimic fairneess is closely conected with antidiscrimination 
and equal protection jurisprudence, specifically the notions 
of "disparate treatment" and "disparate impact".  Any differential treatment that is due to racial animus or discriminatory intent is clearly illegal. But 
the Supreme Court has allowed the use of race in specified contexts, for example, in college admissions for the purpose of fostering diversity (see e.g. [Fisher v. University of Texas (2016)](https://www.oyez.org/cases/2015/14-981)). Evidence of disparate impact against a protected group is enough to make a prima facie case of discrimination. This applies to sectors such as employment and housing (see e.g. [Title VII of the Civil Rights Act of 1964](https://www.eeoc.gov/laws/statutes/titlevii.cfm) and [Hazelwood School District v. United States (1977)](https://en.wikipedia.org/wiki/Hazelwood_School_District_v._United_States)). 
The criminal justice system, however, seems exempt. An elaborate statistical analysis – showing that death penalty decisions in Georgia disproportionately targeted African Americans, controlling for several variables – was not enough to convince the Court that the system violated equal protection (see [McClensky v. Kemp (1987)](https://www.oyez.org/cases/1986/84-6811)). 
It is worth thinking about whether the legal notions of disparate treatment and disparate impact 
can help to understand what algorithmic fairness consists in. 

*Key readings and materials*

- Barocas and Selbst, [Big Data's Disparate Impact](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)

*Additional readings and materials*

- Any court opinion linked above

## Wk 10 - 10/21

**TBA**

## Wk 11 - 10/28

**The Economics of Discrimination**

The academic debate about algorithmic fairness mirrors, in some important ways, 
the debate among economists about different tests for what counts as 
discriminatory behaviour. We will learn the basic of the economic 
approach to discrimination guided by Nobel prize winners [Gary Becker](https://en.wikipedia.org/wiki/Gary_Becker) and
[Kenneth Arrow](https://en.wikipedia.org/wiki/Kenneth_Arrow)

*Key readings and materials*

<<<<<<< HEAD
-  Becker, [The Economic Way of Looking at Life](https://www.nobelprize.org/uploads/2018/06/becker-lecture.pdf) (mostly focus on the sections about discrimination and crime)
=======
-  Becker, [The Economic Way of Looking at Life](https://www.nobelprize.org/uploads/2018/06/becker-lecture.pdf) (focuson the sections about discrimination and crime)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd

- Arrow, [What Has Economics to Say About Racial Discrimination?](http://www.sas.rochester.edu/psc/clarke/214/Arrow98.pdf)

- Stanford Open Policing, [Nationwide Analysis of Traffic Stops and Searches](https://openpolicing.stanford.edu/findings/)

*Additional readings and materials*

- Pierson et al, [A large-scale Analysis of Racial Disparities in Police Stops Across the United States](https://5harad.com/papers/100M-stops.pdf)

<<<<<<< HEAD
-  Knowles,  Persico and  Todd, [Racial Bias in Motor Vehicle Searches: Theory and Evidence](https://www.nber.org/papers/w7449)
=======
-  Knowles,  Persico,  Todd, [Racial Bias in Motor Vehicle Searches: Theory and Evidence](https://www.nber.org/papers/w7449)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd

-  Simoiu,  Corbett-Davies and  Goel, [The Problem of Infra-Marginality](https://5harad.com/papers/threshold-test.pdf)


## Wk 12 - 11/4

**Fairness v. Welfare**

We now analize the question of algorithmic 
fairness from the perspective of utilitarianism. 
Utilitarianism, roughly, says that the right action 
is one that maximizes social welfare. 
If utilitarianism is the correct 
ethical theory, algorithms should maximize 
social welfare and this may 
require to treat people unfairly. 

*Key readings and materials*


- Kaplow and  Shavell, [Fairness Versus Welfare](https://www.nber.org/papers/w9622)

- Huq, [Racial Equity in Algorithmic Criminal Justice](https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=3972&context=dlj)

*Additional readings and materials*

-  Bentham, Introduction to the Principles of Morals and Legislation, 
[Chps. I–II](https://www.econlib.org/library/Bentham/bnthPML.html)


## Wk 13 - 11/11

**Equality** 

Against utilitarianism, [John Rawls](https://plato.stanford.edu/entries/rawls/), 
an influential political philosopher of the 20th century, argued that 
society must give everyone equal opportunities to develop their talents and that 
inequalities in the distribution of goods can be tolerated only if they work 
to the advantage of the worst-off in society. What would algorithmic fairness 
look like in light of Rawls's view about equal opportunities and 
distributive justice? Other influential philosophers, [Elizabeth Anderson](https://www.newyorker.com/magazine/2019/01/07/the-philosopher-redefining-equality) and [Tim Scanlon](https://en.wikipedia.org/wiki/T._M._Scanlon), have also defended  egalitarian views. 
What would algorithmic fairness look like in light of these egaliatarian views?


*Key readings and materials*

- Rawls, A Theory of Justice, 3, 11–12

- Anderson, [What is the Point of Equality?](https://www.philosophy.rutgers.edu/joomlatools-files/docman-files/4ElizabethAnderson.pdf)

- Binns, [Fairness in Machine Learning: Lessons from Political Philosophy](http://proceedings.mlr.press/v81/binns18a/binns18a.pdf)


*Additional readings and materials*


-  Scanlon, [Why Does Inequality Matter?](https://law.yale.edu/sites/default/files/documents/pdf/Intellectual_Life/ltw-Scanlon.pdf) 

<<<<<<< HEAD
-  Kolodny, [Why Equality of Treatment and Opportunity Might Matter](https://philpapers.org/rec/KOLWEO)
=======
- Niko Kolodny, [Why Equality of Treatment and Opportunity Might Matter](https://philpapers.org/rec/KOLWEO)
>>>>>>> 747922a3497a9a88b0c0e630e672e4824842ebcd

-  Heidari,  Loi, Gummadi and Krause, [A Moral Framework for Understanding fair ML Through Economic Models of Equality of Opportunity](https://www.cs.cornell.edu/~hh732/heidari2019moral.pdf)



## Wk 14 - 11/18

**Individualized Decisions** 

Since algorithms often rely on statistical generalizations and correlations, 
some worry that algorithms disregard  the individual circumstances 
of each person. Do people have a right to be treated as individuals? 
What would that mean?

*Key readings and materials*

- Lippert-Rasmussen, [We Are All Different](https://link.springer.com/article/10.1007%2Fs10892-010-9095-6)

*Additional readings and materials*

- Eidelson, [Treating People as Individuals](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2298429)

- Beeghly, [Failing to Treat Persons as Individuals](https://quod.lib.umich.edu/e/ergo/12405314.0005.026/--failing-to-treat-persons-as-individuals?rgn=main;view=fulltext)

## Wk 15 - 11/25

**Thanksgiving, No Class** 

## Wk 16 - 12/1

**TBA**



**Third essay or research paper due**

# Similar courses

- Aaron Fraenkel's [Fairness and Algorithmic Decision Making](https://afraenkel.github.io/portfolio/curric-3-fairness/) - UC, San Diego

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) - book

- Moritz Hardt's [Fairness in Machine Learning](https://fairmlclass.github.io/) - UC, Berkeley

- Arvind Narayanan's [Fairness in Machine Learning](https://docs.google.com/document/d/1XnbJXELA0L3CX41MxySdPsZ-HNECxPtAw4-kZRc7OPI/edit) - Princeton


