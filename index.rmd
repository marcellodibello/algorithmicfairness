---
title: "Algorithmic Fairness"
subtitle: "PHIxyz - Fall 2021"
author: "Marcello Di Bello  - mdibello@asu.edu"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 1
    toc: TRUE
---    




<!-- 
OTHER INITIAL SETTING 

---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 2
    toc: TRUE
---    
    
--->


<!-- 
OTHER INITIAL SETTING PRETTY HTML
---
pagetitle: "Algorithmic Fairness"
title: "Algorithmic Fairness"
subtitle: ""
author: "Marcello Di Bello"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    navbar: yes
    toc: yes
    toc_depth: 3
    toc_float: no
---
-->



<!-- 
---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  html_document:
    toc: true
    navbar: yes
    toc_depth: 3
    toc_float: yes
    theme: cosmo
  pdf_document:
    toc: false
  highlight: zenburn    
---
--->



<script>
   $(document).ready(function() {
     $head = $('#header');
     $head.prepend('<img src=\"algo-fair-logo.png\" style=\"float: right;width: 350px;height: 220px;\"/>')
   });
</script>


<!---
<img src="data-ethics-logo.jpg" style="position:absolute;top:0px;right:0px;" />
--->

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	             cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
      font-family:'Avenir Next';
  }

</style>



<!--- 
ADDED STYLES/ FONT SIZSES
<style type="text/css">
body{ /* Normal  */
      font-size: 18px;
  }
  
 
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>
-->



# Course description

Public and private sector entities rely on algorithms to streamline decisions about 
healthcare, child abuse, public housing, bail and sentencing. 
This course examines the ongoing debate among computer scientists, economists, 
legal scholars and moral philosophers 
about the fairness of algorithmic decision-making.  What does it mean for algorithmic decisions to be fair? 
How does algorithmic fairness relate to other notions such as equal treatment, anti-discrimination and fair equality 
of opportunity? What are the formal tools for measuring fairness? What other values and goals 
should inform the design of ethical algorithms, such as minimizing the harm toward disavatanged minorities or respecting 
each person's individual circumstances?



# Objectives

This course is meant for advanced undergraduates or graduates students. 
Students will become familiar with different conceptions of algorithmic fairness,
such as predictive parity, classification parity and counterfactual fairness, as well as 
explore connections between algorithmic
fairness, anti-discrimination law and philosophical concepts such as fair equality of opportunity. 
Students will also develop critical thinking skills, in reading  and  writing, 
for the analysis of new technologies and the ethical issues they pose. Students will 
read academic papers in different disciplines---computer science, economics, law 
and philosohy---and will develop an appreciation for differences in scholarship 
across disciplines. 

# Similar courses

- Aaron Fraenkel's [Fairness and Algorithmic Decision Making](https://afraenkel.github.io/portfolio/curric-3-fairness/) - UC, San Diego

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) -- book

- Moritz Hardt's [Fairness in Machine Learning](https://fairmlclass.github.io/) - UC, Berkeley

- Arvind Narayanan's [Fairness in Machine Learning](https://docs.google.com/document/d/1XnbJXELA0L3CX41MxySdPsZ-HNECxPtAw4-kZRc7OPI/edit) - Princeton



# Schedule & Readings


## Wk 1 - 8/19:  Introduction

The question of algorithmic fairness is technical (how do algorithms work?), theoretical (what does "fairness" mean?) and practical (how do algorithms impact people's lives?). On the technical side, we will survey key concepts, such as algorithm, machine learning, supervised v. unsupervised learning, regression, classification v. clustering. This survey will be informal. Those who wish to go deeper may consult the additional materials.  On the theoretical side, we will brainstorm ideas about what we mean by "fairness" and how an algorithm could fail to be fair. We will explore several definitions of fairness more deeply later in the course. Finally, on the practical side, we will read stories about people whose lives were affected, often adversely, by algorithms. 


*Key readings and materials*

- O'Neil, [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/), Introduction

- Kearns and Roth, [The Ethical Algorithm](https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205), Chp. 1 

- [A Gentle Introduction to Machine Learning](https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)

- Eubanks, [Automating Inequality](https://www.amazon.com/Automating-Inequality-High-Tech-Profile-Police/dp/1250215781/), Chp. 4


*Additional readings and materials*

- Rogers and Mark Girolami, [A First Course in Machine Learning](https://www.amazon.com/Course-Machine-Learning-Pattern-Recognition-ebook/dp/B01N7ZEBK8), Chapter 1

- Michael Nielsen, Neural Networks and Deep Learning, [Chp 1](http://neuralnetworksanddeeplearning.com/chap1.html)

- [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U)


## Wk 2 - 08/26: ProPublica v. Northpointe

To examine more closely the interplay between technical, theoretical and practical 
questions, we will look at a case study, the algorithm
[COMPAS](https://en.wikipedia.org/wiki/COMPAS_(software)) 
used in criminal justice to help judges make decisions about bail, preventative detention and sentencing. The website of investigative journalism [ProPublica](https://www.propublica.org/) argued that COMPAS is biased against black people. Northpointe (now [Equivant](https://www.equivant.com/)), the company that designed COMPAS, 
rejected the accusation claiming that COMPAS is racially fair. They both provided numbers to back up their claims. You will see that the crux here is a disagreement 
about the meaning of "fairness". So is COMPAS racially biased or not?

In order to understand this debate, you should be familiar with a bif of probability theory and machine learning, in particular, notions such as false positive, false negative, sensitivity, specificity, Area Under the Curve (AUC) and confusion matrix.  We will review these notions as needed.

*Key readings and materials*

- [Machine Learning Fundamentals: Sensitivity and Specificity](https://www.youtube.com/watch?v=vP06aMoz4v8)

- ProPublica, [Machine Bias: There's software used across the country to predict future criminals. And it’s biased against blacks.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

- William Dieterich, Christina Mendoza, and Tim Brennan. [COMPAS risk scales: Demonstrating accuracy equity and predictive parity performance of the COMPAS risk scales in Broward county](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) 

*Additional readings and materials*

- ProPublica, [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

- Anthony W. Flores, Kristin Bechtel, and Christopher T. Lowenkamp. [False positives, false negatives, and false analyses: A rejoinder to “Machine bias”](https://www.uscourts.gov/federal-probation-journal/2016/09/false-positives-false-negatives-and-false-analyses-rejoinder)

- [Technical Flaws of Pretrial Risk Assessments Raise Grave Concerns](https://cyber.harvard.edu/story/2019-07/technical-flaws-pretrial-risk-assessments-raise-grave-concerns)

- [Parole Denied: One Man's Fight Against a COMPAS Risk Assessment](https://www.youtube.com/watch?v=UySPgihj70E)


## Wk 3  - 9/2: (Mis)guided by Data

Let's take a step back and examine the basis of algorithmic decision-making. 
Data are abundant today. Though incomplete and never truly conclusive, data 
can guide our decisions, often saving lives. For example, data have been playing a key role in the response to the COVID-19 pandemic (see, for example, Max Roser, Hannah Ritchie and Esteban Ortiz-Ospina, [Coronavirus Disease (COVID-19) - Statistics and Research](https://ourworldindata.org/coronavirus)). 

Another example, in the context of criminal justice, is the data-driven Public Safety Assessment ([PSA](https://www.psapretrial.org/)) algorithm, now widely used in several jurisdictions in the United States to help judges make pre-trial decisions. According to a [report](https://njcourts.gov/courts/criminal/reform.html) from New Jersey, pre-trial jail population descrease significantly after PSA was adopted. The ACLU of New Jersey [endorsed](https://www.aclu-nj.org/theissues/criminaljustice/pretrial-justice-reform) 
the use of the PSA algorithm because it has the potential to end the pre-trial system of money and bail that disproportionately harms the poor. 

Although data can be helpful to make decisions, they can also be biased, partial or incomplete. Biases in the data often disdavatange minorities. 

*Key readings and materials*

Kearns and Roth, [The Ethical Algorithm](https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205), Chp. 3 (only pp. 57-64)

- Moritz Hardt, [How Big Data Is Unfair](https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de)

- Caroline C. Perez, [Invisible Women: Data Bias in a World Designed for Men](https://www.amazon.com/Invisible-Women-Data-World-Designed/dp/1419729071), Chp. 8



*Additional readings and materials*

- Lazer, Kennedy, King, and Vespignani, [The parable of Google flu: traps in big data analysis](https://gking.harvard.edu/files/gking/files/0314policyforumff.pdf)

- Joy Buolamwini, [Algorithms Aren’t Racist. Your Skin Is just too Dark](https://hackernoon.com/algorithms-arent-racist-your-skin-is-just-too-dark-4ed31a7304b8)


## Wk 4 - 9/9: Feedback Loops

Another problem with data is that, via feedback loops, decisions based on biased data magnify errors on a large scale especially when they are automated. Even unbiased data can produce pernicious feedback loops. We will exmaine examples of feedback loops in alorithms used in education and predictive policing. 


*Key readings and materials*

- Cathy O'Neil, [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/), Chp. 3 

-  Lum and Isaac, [To predict and serve?](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x)


*Additional readings and materials*


- Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkata. [Runaway feedback loops in predictive policing](https://arxiv.org/abs/1706.09847) -- [video](https://www.youtube.com/watch?v=9C6epG-Wyuw)


## Wk 5 - 9/16:  The Meanings of Fairness 

What does it mean for algorithmic decisions to be 
fair in the first place? Do fairness and acciracy cinflict with one another? 
Computer scientists have formulated several different metrics of algorithmic fairness. 
By one count, there are as many as 21 definitions. 
Two of the most important definitions are called: *demographic parity*,  
*classification parity* and *predictive parity*. 
We will examine these definitions and the relantionhip between algorithmic 
fairness and accuracy. 


*Key readings and materials*

- Kearns and Roth, *The Ethical Algorithm*, Chp. 3 

- Sam Corbett-Davies and Sharad Goel. [The measure and mismeasure of fairness: A critical review of fair machine learning](https://arxiv.org/abs/1808.00023)

*Additional readings and materials*

- Arvind Narayanan, [21 fairness definitions and their politics](https://www.youtube.com/watch?v=jIXIuYdnyyk)

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) -- chapter 2 and 4

- Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva, [Counterfactual Fairness](https://arxiv.org/abs/1703.06856)

## Wk 6 - 09/23:  Impossibility Theorems 


It turns out that it is mathematically impossible, under realistic conditions,  for algorithms
to satisfy different conceptions of fairness at the same time. These impossibility results have led scholars to consider trade-offs between different conceptions of fairness or 
be skeptical toward existing definitions.

*Key readings and materials*

- Alexandra Chouldechova, [Fair Prediction with Disparate Impact](https://arxiv.org/pdf/1703.00056.pdf)

- Deborah Hellman, [Measuring Algorithmic Fairness](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3418528)


*Additional readings and materials*

- Grant, [Is It Impossible To Be Fair?](https://jainfamilyinstitute.github.io/algorithmic-fairness/)


- Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. [Fairness in criminal justice risk assessments: The state of the art](https://doi.org/10.1177/0049124118782533) 

- Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan, [Inherent Trade-Offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807) -- [video](https://www.youtube.com/watch?v=K7i_tnflZ64) 



## Wk 7 - 09/30:  Fair risk assessment 

The debate about algorithmic fairness is nothing new. Since the development of statistical methods in insurance and banking, claims of racial and gender discrimination have been advanced against risk assessment tools used by banks and insurance companies to price loans and insurance policies. We will review this history to understand analogies and differences between the fair risk assessment in the 20th century and algorithmic fairness in the 21st century. 

*Key readings and materials*

- Rodrigo Ochigame, [The Long History of Algorithmic Fairness](https://phenomenalworld.org/analysis/long-history-algorithmic-fairness)  

*Additional readings and materials*


**First paper due**


## Wk 8 - 10/7: Structural unfairness 

The definitions  of algorithmic fairness we have seen so far describe the extent to which *actual* rates of error are unevenly (and thus unfairly) allocated across different socially relevant groups. Scholars on computer science have also used more sophisticated definitions to formulate a *counterfactual* or *causal* definition of algorithmic fairness. These definitions attempt to capture the structural dimension of racial discrimination (as in the expression "structural racism"). Other scholars in law, sociology and philosophy have argued these attempt are still unsatisfactory. 

*Key readings and materials*

- Lilly Hu, Disparate Causes, [Part I](https://phenomenalworld.org/analysis/disparate-causes-i) and [Part II](https://phenomenalworld.org/analysis/disparate-causes-pt-ii)


*Additional readings and materials*

-  Barocas, Hardt and Narayanan, Fairness and Machine Learning, [Chp 5 - Causality](https://fairmlbook.org/causal.html)

-  Chiappa and Gillam,  [Path-Specific Counterfactual Fairness](https://arxiv.org/pdf/1802.08139.pdf)

-  Kohler-Hausmann, [Eddie Murphy and the Dangers of Counterfactual Causal Thinking About Detecting Racial Discrimination](https://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1374&context=nulr)

## Wk 9 - 10/14: Algorithmic Equal Protection?

Algorithimic fairneess is closely conected with antidiscrimination 
and equal protection jurisprudence, specifically the notions 
of "disparate treatment" and "disparate impact".  Any differential treatment that is due to racial animus or discriminatory intent is clearly illegal. But 
the Supreme Court has allowed the use of race in specified contexts, for example, in college admissions for the purpose of fostering diversity (see e.g. [Fisher v. University of Texas (2016)](https://www.oyez.org/cases/2015/14-981)). Evidence of disparate impact against a protected group is enough to make a prima facie case of discrimination. This applies to sectors such as employment and housing (see e.g. [Title VII of the Civil Rights Act of 1964](https://www.eeoc.gov/laws/statutes/titlevii.cfm) and [Hazelwood School District v. United States (1977)](https://en.wikipedia.org/wiki/Hazelwood_School_District_v._United_States)). 
The criminal justice system, however, seems exempt. An elaborate statistical analysis – showing that death penalty decisions in Georgia disproportionately targeted African Americans, controlling for several variables – was not enough to convince the Court that the system violated equal protection (see [McClensky v. Kemp (1987)](https://www.oyez.org/cases/1986/84-6811)). 
It is worth thinking about whether the legal notions of disparate treatment and disparate impact 
can help to understand what algorithmic fairness consists in. 

*Key readings and materials*

- Barocas and Selbst, [Big Data's Disparate Impact](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)

*Additional readings and materials*

- Any the court opinions linked above

## Wk 10: TBA

## Wk 11 - 10/28: Testing for discrimination 

The academic debate about algorithmic fairness mirrors, in some important ways, 
the debate among economists about different tests for what counts as 
discriminatory behaviour. 

*Key readings and materials*


- Gary Becker, [The Economic way of Looking at Life](https://www.nobelprize.org/uploads/2018/06/becker-lecture.pdf)


*Additional readings and materials*


- John Knowles, Nicola Persico, Petra Todd, [Racial Bias in Motor Vehicle Searches: Theory and Evidence](https://www.nber.org/papers/w7449)


- Camelia Simoiu, Sam Corbett-Davies and Sharad Goel, [The Problem of Infra-Marginality](https://5harad.com/papers/threshold-test.pdf)


## Wk 12 - 11/4: Fairness v. Welfare

Let's try to analize the question of algorithmic 
fairness from the perspective of utilitarianism. 
Utilitarianism, roughly, says that the right action 
is one that maximizes social welfare overall. 
If utilitarianism is the correct 
ethical theory, algorithms should maximize 
overall social welfare and this may 
require to treat people unfairly. 

*Key readings and materials*


- Louis Kaplow and Steven Shavell, [Fairness Versus Welfare](https://www.nber.org/papers/w9622)

- Aziz Z. Huq, [Racial Equity in Algorithmic Criminal Justice](https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=3972&context=dlj)

*Additional readings and materials*


- Jeremy Bentham, "Introduction to the Principles of Morals and Legislation", 
[Ch. I–II](https://www.econlib.org/library/Bentham/bnthPML.html)


## Wk 13 - 11/11: Equality 

Against utilitarianism, [John Rawls](https://plato.stanford.edu/entries/rawls/), 
an influential political philosopher of the 20th century, argued that 
society must give everyone equal opportunities to develop their talents and that 
inequalities in the distribution of goods can be tolerated only if they work 
to the advantage of the worst-off in society. What would algorithmic fairness 
look like in light of Rawls's view about equal opportunities and 
distributive justice? Other influential philosophers, [Elizabeth Anderson](https://www.newyorker.com/magazine/2019/01/07/the-philosopher-redefining-equality), Tim Scanlon and Niko Kolodny, have also defended  egalitarian views. What would algorithmic fairness 
look like in light of these egaliatarian views?


*Key readings and materials*

- John Rawls, A Theory of Justice, 3, 11–12

- Elizabeth S. Anderson, [What is the Point of Equality?](https://www.philosophy.rutgers.edu/joomlatools-files/docman-files/4ElizabethAnderson.pdf)


*Additional readings and materials*


- T. M. Scanlon, [Why Does Inequality Matter?](https://law.yale.edu/sites/default/files/documents/pdf/Intellectual_Life/ltw-Scanlon.pdf) 

- Niko Kolodny, [Why equality of treatment and opportunity might matter](https://philpapers.org/rec/KOLWEO)


- Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. [A moral framework for understanding fair ML through economic models of equality of opportunity](https://www.cs.cornell.edu/~hh732/heidari2019moral.pdf)

- Reuben Binns, [Fairness in Machine Learning: Lessons from Political Philosophy](http://proceedings.mlr.press/v81/binns18a/binns18a.pdf)


## Wk 14 - 11/18: Individualized decisions 

Since algorithms often rely on statistical generalizations and correlations, 
some worry that algorithms disregard  the individual circumstances 
of each person. Do people have a right to be treated as individuals? 
What would that mean?

- Kasper Lippert-Rasmussen, [We Are All Different](https://link.springer.com/article/10.1007%2Fs10892-010-9095-6)

- Benjamin Eidelson, [Treating People as Individuals](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2298429)

- Erin Beeghly, [Failing to Treat Persons as Individuals](https://quod.lib.umich.edu/e/ergo/12405314.0005.026/--failing-to-treat-persons-as-individuals?rgn=main;view=fulltext)

## Wk 15 - 11/25: Thanksgiving, No Class 

## Wk 16 - 12/1: TBA



**Second paper due**


# Assignments

## Pass/fail assignments (*low stake*)

Every week please write a **one-page précis** of one of the papers assigned for that week. 
The précis should describe: (a) topic of the paper; 
(b) main thesis (or main theses, if there are more than one); 
(c) supporting arguments; (d) objections to these arguments, complications or 
difficulties that the author considers (if any).


## Papers (*high stake*)

There will be two main graded assignments 
for this course, roughly **15 pages** each. 
The **first** will cover Week 1 through 5, the 
**second**  Week 6 through 10.

**1**. Write an imaginary *philosophical dialogue* between two characters impersonating 
ProPublica and Northpointe. Describe very carefully (a) 
ProPublica's accusation that COMPASS is racially biased and Northpointe's response; (b) assess what further technical and philosophical questions about the nature of fairness must be addressed in order to settle the debate. 
Please always defend the claims made in the dialogue 
with careful and reasoned arguments. 

**2**. Write a *philosophical argumentative paper* that compares and contrasts the formal notions of algorithmic fairness (see Week 4 and 5) with either (i) the economic literature on metrics of discrimination (see Week 6), (ii) equal protection jurprisprudence (see Week 7), (iii) the philosophical literature on fairness and equality (See Week 8, 9 and 10). Please always defend the claims you make with careful and reasoned arguments. Check out these [guidelines](http://www.jimpryor.net/teaching/guidelines/writing.html) on how to write a philosophical argumentative essay.


